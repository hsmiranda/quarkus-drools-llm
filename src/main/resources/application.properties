quarkus.log.level=INFO

# Configure Ollama server to use llama 3.1 model
quarkus.langchain4j.ollama.chat-model.model-id=llama3.1
# Choose a low temperature to minimize hallucination
quarkus.langchain4j.ollama.chat-model.temperature=0
# Set timeout to 3 minutes (local LLM can be quite slow)
quarkus.langchain4j.ollama.timeout=180s
# Enable logging of both requests and responses
quarkus.langchain4j.ollama.log-requests=true
quarkus.langchain4j.ollama.log-responses=true

# Same configuration as before, but with higher temperature to allow more variations in password generation
quarkus.langchain4j.ollama.hotmodel.chat-model.model-id=mistral
quarkus.langchain4j.ollama.hotmodel.chat-model.temperature=10.0
quarkus.langchain4j.ollama.hotmodel.timeout=180s
quarkus.langchain4j.ollama.hotmodel.log-requests=true
quarkus.langchain4j.ollama.hotmodel.log-responses=true

# Drools KieBases and KieSession configuration
drools.kbase.mortgageKB.packages=org.hybridai.mortgage
drools.kbase.mortgageKB.ksession=mortgage

drools.kbase.passwordKB.packages=org.hybridai.password
drools.kbase.passwordKB.ksession=password

drools.kbase.refundCalcKB.packages=org.hybridai.refund.calculator
drools.kbase.refundCalcKB.ksession=refundCalc

drools.kbase.refundStateKB.packages=org.hybridai.refund.statemachine
drools.kbase.refundStateKB.ksession=refundState